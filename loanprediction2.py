# -*- coding: utf-8 -*-
"""loanprediction2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UdFxtgiNOcK8MRYxdWue4mfgNiCJ5JaB
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import cross_val_score,StratifiedKFold

def check_outliers(df):
  for col in df.select_dtypes(include=['int64','float64']):
    q3, q1 = np.percentile(df[col], [75 ,25])
    iqr = q3 - q1
    outliers = df[(df[col]>=(q3 + 1.5 * iqr)) | (df[col]<=(q1 - 1.5 * iqr))]
    print(col,'  ', len(outliers))

def remove_outliers(df):
  for col in df.select_dtypes(include=['int64','float64']):
    q3, q1 = np.percentile(df[col], [75 ,25])
    iqr = q3 - q1
    upper = q1 + 1.5 * iqr
    lower = q1 - 1.5 * iqr 
    df[col].mask(df[col] >= upper, upper, inplace=True)
    df[col].mask(df[col] <= lower, lower, inplace=True)

def check_multicollinearity(df2):
  X = df2.select_dtypes(include=['float64'])
  vif_data = pd.DataFrame()
  vif_data["feature"] = X.columns
    
  # calculating VIF for each feature
  vif_data["VIF"] = [variance_inflation_factor(X.values, i)
                            for i in range(len(X.columns))]
    
  print(vif_data)

df = pd.read_csv('loanprediction_data.csv')
df

check_outliers(df)

remove_outliers(df)
check_outliers(df)

df.isna().sum()

for col in df.select_dtypes(include=['object']): 
  df[col].fillna(df[col].mode()[0], inplace=True)
for col in df.select_dtypes(include=['int64','float64']):
  df[col].fillna(df[col].mean(), inplace=True)

df.isna().sum()

df.Property_Area.value_counts()

'''pa = ['Urban', 'Semiurban', 'Rural']
df['Property_Area'] = df['Property_Area'].apply(lambda x: pa.index(x))

df['Education'] = df['Education'].apply(lambda x: df['Education'].unique().tolist()[::-1].index(x))

df['Dependents'] = df['Dependents'].apply(lambda x: df['Dependents'].unique().tolist()[::-1].index(x))
df'''
from sklearn.preprocessing import OrdinalEncoder
depcat = ['3+','2', '1', '0']
oe_dependants = OrdinalEncoder(categories=[depcat])
df['Dependents'] = oe_dependants.fit_transform(df[['Dependents']])
grad = ['Not Graduate', 'Graduate']
oe_grad = OrdinalEncoder(categories=[grad])
df['Education'] = oe_grad.fit_transform(df[['Education']])
prop = ['Rural', 'Semiurban', 'Urban']
oe_prop = OrdinalEncoder(categories=[prop])
df['Property_Area'] = oe_prop.fit_transform(df[['Property_Area']])
df

marr = pd.get_dummies(df['Married'],prefix='mar_')
gend = pd.get_dummies(df['Gender'])
selem = pd.get_dummies(df['Self_Employed'], prefix='selfemp_')
#prop = pd.get_dummies(df['Property_Area'])
#dep = pd.get_dummies(df['Dependents'])
#edu = pd.get_dummies(df['Education'])
df.drop(columns = ['Loan_ID', 'Married', 'Gender', 'Self_Employed'], inplace=True)
df2 = pd.concat([df, marr, gend, selem], axis=1)

df2

check_multicollinearity(df2)

df3 = df2.drop(columns=['Loan_Amount_Term'])
check_multicollinearity(df3)

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

train_dat=df2.drop(columns=['Loan_Status','Loan_Amount_Term'])
y=df2.Loan_Status
x=np.array(train_dat)
print(x.shape)
y=np.array(y)
print(y.shape)
sc=MinMaxScaler()
ip=sc.fit_transform(x)
xtrain,xtest,ytrain,ytest=train_test_split(ip,y,test_size=0.2)



from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score
from sklearn.naive_bayes import GaussianNB
gnb=GaussianNB()
gnb.fit(xtrain,ytrain)  
predgnb=gnb.predict(xtest) 
cmgnb=confusion_matrix(predgnb,ytest)
print('gnb:')
print('confusion matrix:\n', cmgnb)
predgnbprf=classification_report(ytest, predgnb)
print('classification report:\n', predgnbprf)
stratifiedkf=StratifiedKFold(n_splits=5)
score=cross_val_score(gnb,xtrain, ytrain,cv=stratifiedkf)
print("Cross Validation Scores are {}".format(score))
print("Average Cross Validation score :{}".format(score.mean()))

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(max_iter=1000)
logreg.fit(xtrain, ytrain)
predlog = logreg.predict(xtest)
cmlog = confusion_matrix(predlog, ytest)
print('logistic regression:')
print('confusion matrix:\n', cmlog)
predlogprf=classification_report(ytest, predlog)
print('classification report:\n', predlogprf)
stratifiedkf=StratifiedKFold(n_splits=5)
score=cross_val_score(logreg,xtrain, ytrain,cv=stratifiedkf)
print("Cross Validation Scores are {}".format(score))
print("Average Cross Validation score :{}".format(score.mean()))

from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier(n_estimators=1000)
rf.fit(xtrain,ytrain)
predrf=rf.predict(xtest)
cmrf=confusion_matrix(predrf,ytest)
print('randomforest regression:')
print('confusion matrix:\n', cmrf)
predrfprf=classification_report(ytest, predrf)
print('classification report:\n', predrfprf)
stratifiedkf=StratifiedKFold(n_splits=5)
score=cross_val_score(rf,xtrain, ytrain,cv=stratifiedkf)
print("Cross Validation Scores are {}".format(score))
print("Average Cross Validation score :{}".format(score.mean()))

from sklearn.neighbors import KNeighborsClassifier
knnclass = KNeighborsClassifier(n_neighbors=5)
knnclass.fit(xtrain, ytrain)
predknn=knnclass.predict(xtest)
cmknn=confusion_matrix(predknn,ytest)
print('knn:')
print(cmknn)
predknnprf=classification_report(ytest, predknn)
print('classification report:\n', predknnprf)
stratifiedkf=StratifiedKFold(n_splits=5)
score=cross_val_score(knnclass,xtrain, ytrain,cv=stratifiedkf)
print("Cross Validation Scores are {}".format(score))
print("Average Cross Validation score :{}".format(score.mean()))




import pickle
data = {"model": logreg, "oe_dependants": oe_dependants, "oe_grad": oe_grad,
        "oe_prop":oe_prop}
with open('trainedmodel.sav', 'wb') as file:
    pickle.dump(data, file)
    
    
with open('trainedmodel.sav', 'rb') as file:
    data = pickle.load(file)

lr_model = data["model"]
oe_dependants = data["oe_dependants"]
oe_grad = data["oe_grad"]
oe_prop = data["oe_prop"]

y_pred = lr_model.predict(xtest)
y_pred
    
    
    
    